<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Alexander Fengler</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://AlexanderFengler.github.io/feed.xml" />
<link rel="alternate" type="text/html" href="http://AlexanderFengler.github.io" />
<updated>2015-02-17T14:15:45-08:00</updated>
<id>http://AlexanderFengler.github.io/</id>
<author>
  <name>Alexander Fengler</name>
  <uri>http://AlexanderFengler.github.io/</uri>
  <email>fengleralexander@gmail.com</email>
</author>


  

<entry>
  <title type="html"><![CDATA[Fitting Hyperbolic Discounting Functions]]></title>
  <link rel="alternate" type="text/html" href="http://AlexanderFengler.github.io/neuroeconomics/K-Estimation/" />
  <id>http://AlexanderFengler.github.io/neuroeconomics/K-Estimation</id>
  <published>2015-02-07T00:00:00-08:00</published>
  <updated>2015-02-07T00:00:00-08:00</updated>
  <author>
    <name>Alexander Fengler</name>
    <uri>http://AlexanderFengler.github.io</uri>
    <email>fengleralexander@gmail.com</email>
  </author>
  <content type="html">&lt;h1 id=&quot;tutorial-on-how-to-estimate-individual-k-values-hyperbolic-discounting&quot;&gt;Tutorial on how to estimate individual k-values (Hyperbolic Discounting)&lt;/h1&gt;

&lt;p&gt;In the following I will walk you through the complete procedure necessary to estimate the individual discounting parameter when 
assuming hyperbolic discounting. Once established, the procedure applies analogously to other base functions commonly used to describe human intertemporal decision making (namely exponential discounting, alpha-beta discounting). We closely follow the approach taken by Chabris and Laibson (2008)&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; in their paper &lt;em&gt;Individiual laboratory-measured discount rates predict field behavior&lt;/em&gt; published in the &lt;em&gt;Journal of Risk and Uncertainty&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;outline-of-the-general-procedure&quot;&gt;Outline of the general procedure&lt;/h2&gt;

&lt;p&gt;The hyperbolic discounting function we use in the following is captured by the following term.&lt;/p&gt;

&lt;p&gt;&lt;cite&gt; D(t) = 1 / (1 + k*t) &lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;This equation describes the value assigned to a monetary reward, dependent on the associated time-delay (in arbitrary units, may be days, weeks, month, years). If an individual assignes value (utility) to rewards according to this function, it transforms every possible combination of delay and payoff into an individual valuation which in turn can be ranked in terms of preference (therefore intertemporal preferences). &lt;/p&gt;

&lt;p&gt;The free parameter that is utilized to describe individual differences in intertemporal discounting is the k value in the above function (alpha in Chabris and Laibson (2008)&lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;). &lt;/p&gt;

&lt;p&gt;This parameter is usually fit using choice data, which stems from binary decisions over reward-delay combinations. The basic experimental design to extract such choices is to expose experimental subjects to a series of decision screens (or pen-and-paper questionnaire) in which they are asked to decide over a series of two option choice-sets that involve a small immediate reward (SIR) and a large delayed reward (LDR). The delays and absolute amounts of the rewards are varied so that a broad picture on the intertemporal preferences of the individual subject can be extracted from the decisions taken.&lt;/p&gt;

&lt;p&gt;In this case we opt for the Kirby (1997) intertemporal discounting questionnaire, which may be extended for more precision of the k-value estimates.&lt;/p&gt;

&lt;h3 id=&quot;the-data-set&quot;&gt;The Data Set&lt;/h3&gt;

&lt;p&gt;Following the experimental design outlined above, we end up with a data set that contains a decision for either the SIR or LDR (defined above), for each binary choice set. Decisions for the SIR are coded as 0 and decisions for the LDR are coded as 1.  &lt;/p&gt;

&lt;h3 id=&quot;fitting-the-likelihood-function&quot;&gt;Fitting the likelihood function&lt;/h3&gt;

&lt;p&gt;Given our data set we can proceed to estimate the k-value that describes our subject’s set of choices best. We do this by systematically inserting k’s into the function and solving for the respective likelihood of observing our choice data. We then systematically try different k-values to find the one which maximizes the this likelihood (In reality, given programming conventions, we take the the negative of this likelihood in order to solve a &lt;em&gt;minimization&lt;/em&gt; problem). Note the underlying logic. We are trying to find the parameter values that is &lt;em&gt;most likely to generate the observed choice data&lt;/em&gt;. We quantify the goodness of fit (lack of fit) as follows. Given any particular k we calculate the probability that at a particular choice screen, an agent who behaves according to our currently tested k, would have chosen as the experimental subject did. We take the negative log of this probability. We do this for all choice-screens and sum the negative log likelihoods to end up with a final likelihood for a given k. Now we search the space of k’s to find the best fitting k.&lt;/p&gt;

&lt;h3 id=&quot;issues&quot;&gt;Issues&lt;/h3&gt;

&lt;p&gt;Two things need to be noted for completeness. First, there is a certain precision associated with any set of questions that form the intertemporal choice questionnaire. So it is possible that agents who actually possess differing underlying k-values, end up chosing exactly the same options, simply because the difference in k-values is so small that the current set of questions is not able to distinguish them behaviorally. It is helpful here, to ask more than the standard 27 questions included in the Kirby (1997) questionnaire. Second, following the model of Chabris and Laibson (2008)&lt;sup id=&quot;fnref:1:2&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, we are assuming that an agent has preference shocks following a logit function. Chabris and Laibson (2008)&lt;sup id=&quot;fnref:1:3&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; mention that their set of equations assumes &lt;em&gt;unit variance&lt;/em&gt;, an assumptino which we carry on here. Potentially, the variance implicit in the logit function can be taken as a second degree of freedom in the model. &lt;/p&gt;

&lt;h2 id=&quot;coding-step-by-step&quot;&gt;Coding: Step by step&lt;/h2&gt;

&lt;p&gt;So far we outlined the basic logic of our fitting procedure. In what follows I will carry you through the necessary coding steps to derive a best fitting k, given some choice data. &lt;/p&gt;

&lt;h3 id=&quot;basics&quot;&gt;Basics&lt;/h3&gt;

&lt;h4 id=&quot;reading-in-the-questionnaire-data&quot;&gt;Reading in the questionnaire data&lt;/h4&gt;

&lt;p&gt;In the following we assume that a csv-file (“kirby.csv”) with the following columns is in our working directory. &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Order: A column that stores the order of questions asked  &lt;/li&gt;
  &lt;li&gt;SIR: Column storing the small immediate rewards by choice set&lt;/li&gt;
  &lt;li&gt;LDR: Column storing the large delayed rewards by choice set&lt;/li&gt;
  &lt;li&gt;Delay: Column storing the respecitve delays for LDR’s&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We go ahead an read in the csv file.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readtable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;kirby.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;simulating-some-choices&quot;&gt;Simulating some choices&lt;/h4&gt;

&lt;p&gt;Next we are going to simulate some choices, according to some agent that behaves strictly according to our hyperbolic discounting function given some arbitrary k.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;c&quot;&gt;% len is the length of the choice vector (number of choices to be&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;% predicted)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;% We define a new column in qdat that provides the simulated choices &lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;% 0 represents the immediate reward&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;% 1 represents the delayed reward&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimulateChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let’s take a look what exactly happens in the SimulateChoice function. As you can see below,
we follow the simple decision rule that is outlined by Chabris and Laibson (2008)&lt;sup id=&quot;fnref:1:4&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. An agent will only chose the delayed reward if the following condition holds true.&lt;/p&gt;

&lt;p&gt;&lt;cite&gt; Y / (1 + k*t) - X &amp;gt;= 0 &lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;If the discounted value of the delayed reward is equal or above the imediate reward in the same choice set, we choose the delayed reward (coded as 1). Otherwise we chose the immediate reward (coded as 0).&lt;/p&gt;

&lt;p&gt;The function needs as inputs,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the k according to which our simulated agent takes the decisions&lt;/li&gt;
  &lt;li&gt;the SIR of the respective choice set&lt;/li&gt;
  &lt;li&gt;the LDR of the respective choice set&lt;/li&gt;
  &lt;li&gt;the delay associated with the LDR of the specific choice set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We get back a choice output which is either 0 or 1 as specified above.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;choice &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;SimulateChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;k,sir,ldr,delay&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Not used for now&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%p = exp(ldr/(1 + k*delay)) / (exp(sir) + exp(ldr/(1 + k*delay)));&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% By Chabris/Laibson 2008&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% If the right hand side is above 0 we choose the LDR (large delayed&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% reward) // coded as 1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% IF the left hand is is below  0 we choose the SIR (small immediate&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% reward) // coded as 0&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ldr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that the simulations are simplified, in the sense that, as will be clearer below, we skip any notion of logit distribution for the simulations. In case we assume that expressed preferences stem from a logit distribution, we end up with non binary choice probabilities (any value between 0 and 1) which would change the likelihood fitting procedure.&lt;/p&gt;

&lt;p&gt;As our real data will come in form of binary (either 0 or 1) coding, using our approach to simulate choices makes the code down the line directly applicable to real choice data.&lt;/p&gt;

&lt;p&gt;Note however, that without further explanation, our way of simulating choices here has the effect that our k’s as solutions to our minimization problem may not (in our case actually will not) match the k used to generate our data exactly.&lt;/p&gt;

&lt;h4 id=&quot;given-choices-estimate-loglikelihood-for-any-k&quot;&gt;Given Choices, estimate LogLikelihood for any k&lt;/h4&gt;

&lt;p&gt;Now that we have a bunch of choices, we can try to find the hyperbolic discounting function that fits these choices best. We do this by varying the k-value of the discounting function until we find a global minimium in the negative loglikelihood function.
Lets assume we have a function that takes in as parameters,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The set of real choices&lt;/li&gt;
  &lt;li&gt;The associated SIR’s, LDR’s and delays&lt;/li&gt;
  &lt;li&gt;The k-value we want to try as a choice of parameter&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an output we will get a single number, which is the likelihood. Lets call the function &lt;em&gt;GenerateLogLik&lt;/em&gt; and treat it as a blackbox for now. Such a function is also commonly called an &lt;em&gt;objective or loss function&lt;/em&gt; and our goal is it to minimize its output value. &lt;/p&gt;

&lt;p&gt;We could try out a bunch of k-values by hand, however &lt;em&gt;Matlab&lt;/em&gt; offers us the &lt;em&gt;fminbnd() function&lt;/em&gt; which will vary the k-value outomatically for us to find the optimal k. We just provie a &lt;em&gt;maximum and minimum k&lt;/em&gt; to be tested and the rest will be taken care of. The function will provide us with two output numbers. &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The optimal k-value (parameter value that minimized current loss function)&lt;/li&gt;
  &lt;li&gt;The value of the loss function at optimal k&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The corresponding code looks like this.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;c&quot;&gt;% Run minimization to find k-value&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fminbnd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GenerateLogLik&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now lets take a look at what happens in the &lt;em&gt;GenerateLogLik()&lt;/em&gt; function. Note that within the function, we refer to &lt;em&gt;struct qdat&lt;/em&gt; which is where we stored our questionnaire and choice data earlier. We assume that this struct is assigned as a &lt;em&gt;globally accessable variable&lt;/em&gt;. I will show the full script further below so that the connection of all the code snippets becomes clearer.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;sumloglik &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;GenerateLogLik&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;cur_k&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;&lt;/span&gt;
&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;% Define vector that will store the probability that the model chooses&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;% as the participant for every choice&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;choiceprobabilities&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
   
   &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;
       &lt;span class=&quot;c&quot;&gt;% load the choice probability vector for every choice&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;choiceprobabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetPChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
   
   &lt;span class=&quot;c&quot;&gt;% take sum of logs and negative to be able to work within minimization&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;% framework&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;sumloglik&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choiceprobabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;putting-everything-together&quot;&gt;Putting everything together&lt;/h4&gt;

&lt;p&gt;Once, all of what we discussed before as one function &lt;em&gt;findK()&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;c&quot;&gt;% Provided a certain k for simulating choices, this function return the k&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% that minimizes errors for subjects that have preferences according to a&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% logit destribution with unit variance&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;[x,y] &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;findK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;k&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% READING IN DATA %%%%%%%&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% Initialize Questionnaire Data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Four columns:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% 1. Order &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% 2. SIR (small immediate reward) &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% 3. LDR (large delayed reward) &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% 4. Delay&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readtable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;kirby.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% SIMULATING CHOICES FOR GIVEN K     %&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% Generate / Simulate a vector of decisions for given choice set&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Simplest version:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Solve Y / (1+k*time) - X &amp;gt;= 0 ? if bigger 0 than choose delayed&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% otherwise immediate&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% len is the length of the choice vector (number of choices to be&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% predicted)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% We define a new column in qdat that provides the simulated choices &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% 0 represents the immediate reward&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% 1 represents the delayed reward&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimulateChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%     DEFINE OBJECTIVE FUNCTION         %&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;sumloglik &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;GenerateLogLik&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;cur_k&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;&lt;/span&gt;
&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;% Define vector that will store the probability that the model chooses&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;% as the participant for every choice&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;choiceprobabilities&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
   
   &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;
       &lt;span class=&quot;c&quot;&gt;% load the choice probability vector for every choice&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;choiceprobabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetPChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
   
   &lt;span class=&quot;c&quot;&gt;% take sum of logs and negative to be able to work within minimization&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;% framework&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;sumloglik&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choiceprobabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% RECOVER K WHEN HAVING SET OF CHOICES %&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% Run minimization to find k-value&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fminbnd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GenerateLogLik&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;further-notes-on-k-precision&quot;&gt;Further notes on k-precision&lt;/h3&gt;

&lt;p&gt;Now that we know how to estimate the optimal k-value for a given choice data set, we can follow up with some further considerations on the realtionship between the questionnaire utilized and the precision of the optimal k-values, given binary choice data.
The following code snippet uses the function &lt;em&gt;findK()&lt;/em&gt; defined above and plots the optimal k-values found, as a function of the k-values used to simulate the choice set. &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;ks&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimk&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;findK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Looking at the graph that this script produces, note that it follows what looks like a step function. This illustrates the insensitivity of the simple 27 item Kirby (1997) questionnaire, to small changes in k-values. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://AlexanderFengler.github.io/blog_images/2015-02-07-K-Estimation/kfitted_vs_simulated.png&quot; alt=&quot;kfitted_vs_simulated&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another way to show this, is by simply looking at the generated choices when simulating with varying k-values. It can easily be seen that small changes in k simply do not change any choice in the set and therefore will be treated as stemming from an equivalent agent when we try to find the best fitting k with our likelihood method.&lt;/p&gt;

&lt;p&gt;Changing the underlying questionnaire will have an impact on this sensitivity.&lt;/p&gt;

&lt;h3 id=&quot;where-to-find-all-the-codehttpsgithubcomalexanderfenglerhyperbolicdiscountingmatlab&quot;&gt;Where to find all the &lt;a href=&quot;https://github.com/AlexanderFengler/hyperbolic_discounting_Matlab&quot;&gt;code&lt;/a&gt;?&lt;/h3&gt;
&lt;p&gt;All code is uploaded &lt;a href=&quot;https://github.com/AlexanderFengler/hyperbolic_discounting_Matlab&quot;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;. You can simply download the folder and add it to your path in Matlab.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.nber.org/papers/w14270.pdf&quot;&gt;Chabris C., Laibson D. (2008). Individual laboratory-measured discount rates predict field behavior. &lt;em&gt;J Risk Uncertain&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;,237-269&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;

  &lt;p&gt;&lt;a href=&quot;http://AlexanderFengler.github.io/neuroeconomics/K-Estimation/&quot;&gt;Fitting Hyperbolic Discounting Functions&lt;/a&gt; was originally published by Alexander Fengler at &lt;a href=&quot;http://AlexanderFengler.github.io&quot;&gt;Alexander Fengler&lt;/a&gt; on February 07, 2015.&lt;/p&gt;</content>
</entry>


  

<entry>
  <title type="html"><![CDATA[DDM: Does timestep size affect noise parameter?]]></title>
  <link rel="alternate" type="text/html" href="http://AlexanderFengler.github.io/neuroeconomics/DDM-Timestep-Noise-Behavior/" />
  <id>http://AlexanderFengler.github.io/neuroeconomics/DDM-Timestep-Noise-Behavior</id>
  <published>2015-01-30T00:00:00-08:00</published>
  <updated>2015-01-30T00:00:00-08:00</updated>
  <author>
    <name>Alexander Fengler</name>
    <uri>http://AlexanderFengler.github.io</uri>
    <email>fengleralexander@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;My daily work in the laboratory deals with fitting drift diffusion models to behavioral data (reaction times, decisions) from simple choice experiments. While we also work with more elaborate verisons of the model for which this does not necessarily hold, the standard drift diffusion model has two key parameters. &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The drift rate &lt;em&gt;d&lt;/em&gt; which provides a fixed state-change in the integrator by time/integration step&lt;/li&gt;
  &lt;li&gt;The noise &lt;em&gt;s&lt;/em&gt; (normal gaussian) which is added at each time/integration step&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Essentially the model implements what is called a wiener-process, with a fixed offset by timestep, as described this is the &lt;em&gt;drift rate&lt;/em&gt;. When simulating the model, we essentially track the state of the integrator at each timestep/integrator step and if the state becomes equal or bigger than 1 or equal or smaller -1, we register a choice for an item x or y at the given reaction time (translating model-timesteps into milliseconds).&lt;/p&gt;

&lt;p&gt;Now, as these fitting procedures are computationally quite costly, as easy way to reduce the number of steps in system is by making the model-timesteps bigger as measured in real time. Instead of calculating a new state of the integrator by steps of one millisecond, timesteps of two, five, ten or more milliseconds can be used. This changes the system on two accounts. First, it introduces some imprecision in the outcome space (predicted reactions times) where results can naturally only be binned according to timestates that are actually tracked in the simulation process (e.g. instead or outcomes of 1000ms,1001ms,1002ms we can now only observe 1000ms,1010ms,1020ms). Second, and more intricate, making timestpes more coarse will have an influence on the optimal noise and drift parameters of the integrators. &lt;/p&gt;

&lt;p&gt;This is important, because, while researchers may use algorithms of different temporal resolution, they still want to compare their set of optimal parameters with what has been reported elsewhere (either based on the same experimental paradigm or also across paradigms). In this post I will try to walk through a process of figuring out for myself the changes in the noise term that appear when simulating realtively more timesteps. I ignore the drift rate in the following by assuming there is zero drift rate in the model.
In a follow-up post I may solve for bost, the drift rate and the noise parameter to see whether the drift may change non-trivially (non-linearly).&lt;/p&gt;

&lt;h3 id=&quot;visualizing-the-basic-setup&quot;&gt;Visualizing the basic setup&lt;/h3&gt;

&lt;p&gt;The graphic below shows the basic scenario that I am investigating in this post. It already hints at the fact that keeping the noise in the integrator process constant will increase the spread of the final states with increasing number of steps of integration. This fact is illustrated by the red (end states after one step of integration) and blue (end states with the same integrator noise parameter after five steps of integration) density functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../blog_images/2015-01-30_ddm_parameter_behavior/ddm_timesteps_illustrationmall.png&quot; alt=&quot;&quot; /&gt; &lt;/p&gt;

&lt;p&gt;In what follows, I will demonstrate what happens to the noise parameters, when we increase the temporal resolution of our algorithm, but condition &lt;em&gt;s&lt;/em&gt; (the noise parameter of the integrator process) to generate exactly the same distribution of &lt;em&gt;end states&lt;/em&gt;  that was generated by our &lt;em&gt;reference s&lt;/em&gt; in our reference temporal resolution. As mentioned above, I am going to ignore any effect on the drift rate, which I assume to be simply linear (if 1 at timesteps of one millisecond, then 10 at timesteps of ten milliseconds).&lt;/p&gt;

&lt;h3 id=&quot;the-procedure-has-three-basic-steps&quot;&gt;The procedure has three basic steps:&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;First&lt;/em&gt;, we pick a &lt;em&gt;reference s&lt;/em&gt; (arbitrary) for our reference timestep (we simply assume the base model to have one step). In our case &lt;em&gt;s&lt;/em&gt; is 1. Now from this we know the probability density function (trivially a gaussian normal distribution with mean equal to 0 and standard deviation equal to 1) for all potential end states of the model after one timestep.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second&lt;/em&gt;, now we need an implementation of the diffusion process which we are going to simulate. We do this simulation with a varying number of timesteps. Moreover, for each round of simulations for which any &lt;em&gt;s&lt;/em&gt; for the integrator per step has been picked, we can solve for the standard deviation of the density of end states. Having this standard deviation, we can move to part three.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Third&lt;/em&gt;, for &lt;em&gt;each number of steps&lt;/em&gt; in the process (from 1 to 10 implemented here), we can now solve for the &lt;em&gt;s&lt;/em&gt; we need to apply in the drift diffusion process, to get exactly the same distribution (guassian normal with mean equal zero and standard deviation equal to zero) of possible end-states of the accumulator as in the &lt;em&gt;one step case&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;coding&quot;&gt;Coding&lt;/h3&gt;
&lt;p&gt;In the following I am showing a fully reproducable r-code to illustrate this procedure.&lt;/p&gt;

&lt;h5 id=&quot;loading-necessary-packages&quot;&gt;Loading Necessary Packages&lt;/h5&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;kn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;ggplot2&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# for plotting results&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;neldermead&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# for solving the minimization &lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;tidyr&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# for some data manipulation&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h5 id=&quot;functions-used&quot;&gt;Functions used&lt;/h5&gt;

&lt;p&gt;Implementation of the drift diffusion process:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# The function takes only one input which is the standard deviation of the noise&lt;/span&gt;
  SimpleDiffusion &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# Initialize output vector with length equal to number of simulation runs&lt;/span&gt;
  out &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  &lt;span class=&quot;c1&quot;&gt;# Now for each simulation run&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;i &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# And within run for each step up to step.num (existing global variable at the point of execution)&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;j &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;step.num&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# We caluculate the state of the accumulator&lt;/span&gt;
      out&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; out&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; rnorm&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;std&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# We return the vector with all final states&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;out&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Having the output vector we need to find the best estimate for &lt;em&gt;s&lt;/em&gt; that describes the distribution of outcome states.&lt;/p&gt;

&lt;p&gt;The next function is used to compute a log-likelihood for a given &lt;em&gt;s&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Function takes as input a standard deviation &amp;quot;sd.var.inside&amp;quot; &lt;/span&gt;
LogLikDnorm &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sd.var.inside&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# negative standard deviation is not allowed and will provide huge outcome value&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sd.var.inside &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    LogLik&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9999999&lt;/span&gt; 
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# if valid (positive) standard deviation provided, we calculate the real log-likelihood &lt;/span&gt;
      out &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; dnorm&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;vec&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;sd.var.inside&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      LogLik &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;out&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# out is available in global environment by the time called, comes from SimpleDiffusion()&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# return log-likelihood for given standard deviation &lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;LogLik&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Lastly we need a loss-function that tells us how bad we are doing with our current choice of &lt;em&gt;s&lt;/em&gt; for the given number of steps in approaching the original outcome distribution
based on one timestep.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Takes as input a standard deviation&lt;/span&gt;
LossFun &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Same as above, only positive standard deviation are treated as valid&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    std.loss &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9999999&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std.loss&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# We get the outcome of the diffusion process stored in global variable &amp;quot;vec&amp;quot; taking as input integrator noise standard deviation &amp;quot;std&amp;quot;&lt;/span&gt;
      vec &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;-&lt;/span&gt; SimpleDiffusion&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# Now we search for the standard deviation in the integrator end states that fits the &amp;quot;vec&amp;quot; data best&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# by applying fminsearch&lt;/span&gt;
      sol &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; fminsearch&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;LogLikDnorm&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      
      &lt;span class=&quot;c1&quot;&gt;# we extract the standard deviation that fits the simulated integrator end states best from all fminsearch output&lt;/span&gt;
      std.opti &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;as.vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sol&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;optbase&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;xopt&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# and calculate a loss relative to our reference standard deviation for integrator end states steming from the case where we only use&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# one step of accumulation&lt;/span&gt;
      std.loss &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std.opti &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; std.fixed&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# Returning loss for fminsearch to apply new input noise integrator standard deviation&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;std.loss&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;running-the-model&quot;&gt;Running the model&lt;/h4&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Run optimization and get outcomes&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Define our reference integrator noise standard deviation&lt;/span&gt;
std.fixed &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Initialize the amount of steps that we want ot actually test&lt;/span&gt;
steps &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Initialize an output data.frame&lt;/span&gt;
solutions &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;steps &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;best.sd &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Run optimization&lt;/span&gt;
&lt;span class=&quot;kr&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;i &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; steps&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  step.num &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;-&lt;/span&gt; i
  sol.full &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; fminsearch&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;LossFun&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;step.num&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  solutions&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;best.sd&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;solutions&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;steps &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; i&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;as.vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sol.full&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;optbase&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;xopt&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# Plot Output&lt;/span&gt;
p &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;solutions&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;steps&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;best.sd&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; theme_bw&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;base_size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_line&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;linetype&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;dashed&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;blue&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;size &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;shape &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; ggtitle&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Optimal SD by number Steps&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
p&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../blog_images/2015-01-30_ddm_parameter_behavior/optimal_sd.png&quot; alt=&quot;&quot; /&gt; &lt;/p&gt;

&lt;p&gt;We can see that the standard deviation for the noise of the integrator, seems to follow something like a sqrt() function when we condition the distribution of endstates of the integrator to be steming from a constant distribution. &lt;/p&gt;

&lt;h4 id=&quot;an-alternative-approach-to-illustrate-the-phenomenon&quot;&gt;An alternative approach to illustrate the Phenomenon&lt;/h4&gt;

&lt;p&gt;Lastly, as an addition to aid understanding, it is also possible to simply plot the endstate-distribution for differing numbers of steps, given that the standard deviation of the integrator noise stays constant.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Define out vector&lt;/span&gt;
out  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;steps.1 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            steps.2 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            steps.3 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            steps.4 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            steps.5 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# Run Diffusion process for differing timesteps and collect outputs in data frame&lt;/span&gt;


&lt;span class=&quot;kr&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;i &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;
  step.num &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; i
  out&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;steps.&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;i&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;sep&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; SimpleDiffusion&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Bring data to long format for convenient plotting&lt;/span&gt;
out &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; gather&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;out&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;steps&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;values&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot Results&lt;/span&gt;
p.2 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;out&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;values&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;..&lt;/span&gt;density..&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;fill&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;steps&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; theme_bw&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;base_size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_density&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;alpha&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; ggtitle&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;End-State Distributions&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; ylab&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Density&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; xlab&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;End States&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; scale_fill_manual&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;labels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;3&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;4&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;5&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
p.2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../blog_images/2015-01-30_ddm_parameter_behavior/same_sd_over_steps.png&quot; alt=&quot;&quot; /&gt; &lt;/p&gt;

&lt;p&gt;We can see that the distribution of end states gets wider and wider with increasing number of steps of the integrator. Note that &lt;em&gt;s&lt;/em&gt; of the noise in the integrator process is constant. &lt;/p&gt;

&lt;p&gt;This analysis should provide some intuition about how to convert parameters from drift diffusion processes across different algorithm specifications regarding timestep-sizes of the integrators. &lt;/p&gt;

&lt;p&gt;You can find all code and the markdown file of this post in this &lt;a href=&quot;https://github.com/AlexanderFengler/ddm_noise_parameter_by_steps&quot;&gt;github repository&lt;/a&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://AlexanderFengler.github.io/neuroeconomics/DDM-Timestep-Noise-Behavior/&quot;&gt;DDM: Does timestep size affect noise parameter?&lt;/a&gt; was originally published by Alexander Fengler at &lt;a href=&quot;http://AlexanderFengler.github.io&quot;&gt;Alexander Fengler&lt;/a&gt; on January 30, 2015.&lt;/p&gt;</content>
</entry>

</feed>